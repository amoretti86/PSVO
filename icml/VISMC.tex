%%%%%%%% ICML 2019 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{mathrsfs}
\usepackage{amsmath, amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{matrix}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2019} with \usepackage[nohyperref]{icml2019} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2019}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2019}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\balg}{\begin{align}}
\newcommand{\ealg}{\end{align}}
\newcommand{\bit}{\begin{itemize}}
\newcommand{\eit}{\end{itemize}}
\newcommand{\trm}[1]{\textrm{#1}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\mcl}[1]{\mathcal{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\msc}[1]{\mathscr{#1}}
\newcommand{\lrpar}{\stackrel{\leftrightarrow}{\partial}}
\newcommand{\lpar}{\stackrel{\leftarrow}{\partial}}
\newcommand{\rpar}{\stackrel{\rightarrow}{\partial}}
\newcommand{\tcr}[1]{\textcolor{red}{#1}}
\newcommand{\tcb}[1]{\textcolor{blue}{#1}}
\newcommand{\tcg}[1]{\textcolor{green}{#1}}
\newcommand{\N}{\mcl{N}}

\newcommand{\bx}{\mbf{x}}
\newcommand{\bX}{\mbf{X}}
\newcommand{\bw}{\mbf{w}}
\newcommand{\bW}{\mbf{W}}
\newcommand{\bZ}{\mbf{Z}}
\newcommand{\bP}{\mbf{P}}
\newcommand{\bz}{\mbf{z}}
\newcommand{\baa}{\mbf{a}}
\newcommand{\bY}{\mbf{Y}}
\newcommand{\bfy}{\mbf{y}}
\newcommand{\bth}{\bm{\theta}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\pvp}{\phi,\varphi}
\newcommand{\vph}{\varphi}
\newcommand{\bR}{\mbf{R}}
\newcommand{\bM}{\mbf{M}}
\newcommand{\bS}{\mbf{S}}
\newcommand{\bC}{\mbf{C}}
\newcommand{\nn}{\nonumber}

\newcommand{\vtw}{\vspace{.2cm}}
\newcommand{\vth}{\vspace{.3cm}}
\newcommand{\vsx}{\vspace{.6cm}}
\newcommand{\nwp}[1]{\vth \tbf{{#1}.}}

\newcommand{\tr}[1]{\trm{Tr}\left[ {#1} \right]}



\icmltitlerunning{Submission and Formatting Instructions for ICML 2019}

\begin{document}

\twocolumn[
\icmltitle{Smoothing Variational Objectives with Sequential Monte Carlo\\ for Nonlinear Dynamics}% \\
           %International Conference on Machine Learning (ICML 2019)}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2019
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aeiau Zzzz}{equal,to}
\icmlauthor{Bauiu C.~Yyyy}{equal,to,goo}
\icmlauthor{Cieua Vvvvv}{goo}
\icmlauthor{Iaesut Saoeu}{ed}
\icmlauthor{Fiuea Rrrr}{to}
\icmlauthor{Tateu H.~Yasehe}{ed,to,goo}
\icmlauthor{Aaoeu Iasoh}{goo}
\icmlauthor{Buiui Eueu}{ed}
\icmlauthor{Aeuia Zzzz}{ed}
\icmlauthor{Bieea C.~Yyyy}{to,goo}
\icmlauthor{Teoau Xxxx}{ed}
\icmlauthor{Eee Pppp}{ed}
\end{icmlauthorlist}

\icmlaffiliation{to}{Department of Computer Science, Columbia University, New York, NY}
\icmlaffiliation{goo}{Googol ShallowMind, New London, Michigan, USA}
\icmlaffiliation{ed}{School of Computation, University of Edenborrow, Edenborrow, United Kingdom}

\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}
\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
%This document provides a basic paper template and submission guidelines.
%Abstracts must be a single paragraph, ideally between 4--6 sentences long.
%Gross violations will trigger corrections at the camera-ready phase.
Conductance based models of excitable cells are widely used in computational neuroscience to characterize the spiking activity of individual neurons. Recovering the multidimensional nonlinear dynamics that govern a cell from a single observation is a challenging problem motivating the development of novel techniques in time series analysis. Sequential Monte Carlo methods have been used to construct objective functions for variational inference on time series to perform simultaneous model inference and learning. We develop smoothed variational objectives analogous to forward-backwards message passing. We demonstrate that the use of information from the full time ordered sequence of observations improves both the state estimation and the dynamics learned. Experiments show that this method compares favorably against state of the art methods for variational inference in nonlinear dynamical systems. \textbf{TL;DR summary of paper}
\end{abstract}

\section{Introduction}
\label{submission}
\textbf{motivate the statistical problem with neuroscience question. introduce the method and outline the paper.}
Conductance based models of excitable cells are widely used in computational neuroscience to characterize the spiking activity of individual neurons. Recovering the multidimensional nonlinear dynamics that govern a cell from a single observation is a challenging problem motivating the development of novel techniques in time series analysis. Recently Sequential Monte Carlo methods have been used to construct objective functions for variational inference on time series to perform simultaneous model inference and learning. We develop smoothed variational objectives analogous to forward-backwards message passing. We demonstrate that the use of information from the full time ordered sequence of observations improves both the state estimation and the dynamics learned. Experiments show that this method compares favorably against state of the art methods for variational inference in nonlinear dynamical systems.

\textbf{talk about dimensionality expansion, dimensionality reduction, prediction.}

\section{Related Work}
Discuss SMC based methods including AESMC, VSMC, FIVO. 

Contrast with pure VI methods including VRNN, DKF, LFADS, GfLDS, VIND. 


\section{Theory}
\subsection{Sequential Monte Carlo}
Give a brief review of SMC and VI. Why is this approach promising as opposed to pure VI?

SMC methods factorize an ordered sequence of observations $\bX \equiv \{\bx_1,\dots\bx_T\}$, $\bx_t\in\mbb{R}^{d_X}$ governed by an ordered a sequence of latent variables $\bZ\equiv\{\bz_1,\dots\bz_T\}$, $\bz_T\in \mbb{R}^{d_Z}$ that evolve according to stochastic dynamics. The target distribution $\{p_{\theta}(\bz_t|\bx_t)\}_{t=1}^{T}$ is assumed intractable due to 

SMC factorizes the target distribution $\{p_{\theta}(\bz_t|\bx_t)\}_{t=1}^{T}$ into a sequence of distributions of increasing spaces. 


\subsection{Auto-Encoding Sequential Monte Carlo}
Recap main idea from AESMC and FIVO. 
\be
\log p(\bX) = \log \int p(\bX, \bZ) d\bZ = \log \frac{p(\bX, \bZ)}{p(\bZ|\bX)} \,. \label{ll}
\ee


\be
\log p(\bX) \geq \msc{L}_{\trm{ELBO}}(\bX) = \underset{q}{\mbb{E}}[\log p(\bX, \bZ)] - \underset{q}{\mbb{E}} [\log q(\bZ|\bX)] \label{ELBO} \,.
\ee


\be
\hat{\mathcal{Z}}_{SMC} \equiv \prod\limits_{t=1}^{T}\Big[\frac{1}{N}\sum\limits_{n=1}^{N}w_t^{(n)} \Big] 
\ee


Importance Weighted Auto Encoders have been extended to the time series setting in the design of Sequential Monte Carlo (SMC) algorithms~\cite{DBLP:conf/aistats/NaessethLRB18, NIPS2017_7235, anh2018autoencoding}. SMC methods factorize the target distribution into a distribution of increasing spaces by performing importance sampling sequentially. Define importance weights $w_t^{(k)}$ at time $t$ for sample $k$ as follows:
\begin{align}
w_t^{(k)} &\equiv \frac{f(\bz_t^{(k)}|\bz_{t-1}^{(k)})g(\bx_t|\bz_t^{(k)})}{q(\bz_t^{(k)}|\bz_{t-1}^{(k)},\bx_t)} \\
\intertext{where $\bz_t^{(k)}$ is sampled:} 
\bz_t^{(k)} &\sim q(\bz_t^{(k)}|\bz_{t-1}^{(k)},\bx_t)
\end{align}
Various resampling schemes exist so that the samples which are referred to as particles are focused on promising regions of state space. This can be achieved by resampling particles according to their importance sampling weights:
\begin{align}
\baa_{t-1}^{k} &\sim \text{Discrete}(\cdot|w_{t-1}^{(1)}, \cdots, w_{t-1}^{(K)})  \\
w_t^k &\equiv \frac{f(\bz_t^{(k)}|\bz_{t-1}^{\baa_{t-1}^k})g(\bx_t|\bz_t^{\baa_{t-1}^k})}{q(\bz_t^{k}|\bz_{t-1}^{\baa_{t-1}^k},\bx_t)} 
\end{align}
At the last time step, we can evaluate the posterior as $\sum\limits_{k=1}^{K}\bar w_T^k\delta_{\bZ_{1:T}^k}(\bZ_{1:T})$ averaging over paths to compute the functional integral. SMC also gives an unbiased estimate for the marginal likelihood:
\be
\hat{\mathcal{Z}}_{SMC} \equiv \prod\limits_{t=1}^{T}\Big[\frac{1}{K}\sum\limits_{k=1}^{K}w_t^{(k)} \Big] 
\ee

An important insight of \cite{NIPS2017_7235, anh2018autoencoding} is that the SMC algorithm is deterministic conditioning on $(\bZ_{1:T}^{(1:K)}, \mbf{A}_{1:T-1}^{(1:K)})$. The proposal can thus be reparameterized to act as a variational distribution that can be encoded:
\begin{align}
Q_{SMC}&(\bZ_{1:T}^{1:K}, \mbf{A}_{1:T-1}^{1:K}) \equiv \Bigg(\prod\limits_{k=1}^{K}q_{1,\phi}(\bz_1^k) \Bigg) \\
\times &\Bigg(\prod\limits_{t=2}^{T}\prod\limits_{k=1}^{K}q_{t,\phi}(\bz_t^k|\bz_{1:t-1}^{\baa_{t-1}^k})\cdot\text{Discrete}(\baa_{t-1}^k |\mbf{w}_{t-1}^{1:K}) \Bigg) \nonumber
\end{align}
This gives a way of constructing a cost function for simultaneous model inference and learning. The cost is constructed by running SMC and performing stochastic gradient ascent on the importance weighted ELBO:
\begin{align}
 \msc{L}_{\trm{ELBO SMC}}&(\theta, \phi, \bZ_{1:T}) \equiv \int Q_{SMC}(\bZ_{1:T}^{1:K}, \mbf{A}_{1:T-1}^{1:K})\\
 \times &\log \hat{\mathcal{Z}}_{SMC}(\bZ_{1:T}^{1:K}, \mbf{A}_{1:T-1}^{1:K})d\bZ_{1:T}^{1:N}d\mbf{A}_{1:T}^{1:K} \nonumber
\end{align}
Unlike pure variational methods, this does not involve inverting a block-tridiagonal matrix which mixes components of state space through the covariance. Information from the complete data $\bX_{1:T}$ is not used to the future of $t<T$ to infer $\bz_t$.

\subsection{Smoothing Variational Objectives}
Emphasize novelty of the proposed model, novelty of the experiments and the results. Discuss the implementation details. 

\textbf{The big question to answer is, how is this different from AESMC or FIVO in terms of theory? what is unique with respect to the applications and results?} 

nonlinear time invariant function learned on a smoothed SMC objective.


\section{Results}
Define quantitative evaluation metric. Motivate the use of k-step $MSE_k$ and k-step $R^2_k$.
\be
\trm{MSE}_k =  \sum_{t=0}^{T-k} \left( \bx_{t+k} - \hat{\bx}_{t+k} \right)^2 \,,\quad R^2_k = 1 - \frac{k\trm{MSE}}{\sum_{t=0}^{T-k} \left( \bx_{t+k} - \bar{\bx} \right)^2}
\ee

\subsection{Fitzhugh Nagumo}

\begin{align}
\dot V &= f(V) - W + I_{ext} \,, \nn\\
\dot W &= a(bV - cW)
\end{align}

\begin{figure*}
\includegraphics[width=\columnwidth]{}
\caption{Summary of the Fitzhugh Nagumo results: (left) latent dynamics and paths for the original system (center) inferred 2D dynamics and paths from a noisy 1D observation (right) $R^2_k$ for various models.}
\end{figure*}


\subsection{Lorenz Attractor}
\begin{align}
  \dot{z}_1 & = \sigma(z_2-z_1) \,, \nn \\
  \dot{z}_2 & = z_1(\rho - z_3) - z_2 \,, \\
  \dot{z}_3 & = z_1z_2 - \beta z_3\,. \nn
\end{align}
\begin{figure*}
\includegraphics[width=\columnwidth]{}
\caption{Summary of the Lorenz results: (left) latent paths for the original system (center) inferred paths from a noisy 10D observation (right) $R^2_k$ for various models}
\end{figure*}

\subsection{Single Cell Recordings}

\begin{figure*}
\includegraphics[width=\columnwidth]{}
\caption{}
\end{figure*}

\section{Discussion}


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{*}

\bibliography{vismc}
\bibliographystyle{icml2019}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DELETE THIS PART. DO NOT PLACE CONTENT AFTER THE REFERENCES!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
